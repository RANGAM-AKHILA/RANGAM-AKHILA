{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RANGAM-AKHILA/RANGAM-AKHILA/blob/main/Copy_of_AIML_Tutorial_Training_a_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is a Neural Network\n",
        "\n",
        "---\n",
        "\n",
        "Neural networks are a set of algorithms inspired by the functioning of the human brain. When you open your eyes, the information you perceive, known as data, is processed by neurons, which are the data processing cells in your brain. These neurons recognize patterns in the data and enable you to identify and understand your surroundings.\n",
        "\n",
        "Here is an example of Neural Network trying to predict the image data that given to it(the image data is 28x28 pixels, thats 784 pixels at input neurons). it predicts that the no is 2 here:\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:640/format:webp/0*aWIO7eB6E4-cIkK9.gif)\n",
        "\n"
      ],
      "metadata": {
        "id": "TVPFp2Bx-BJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Breaking Down The Neural Network!\n",
        "\n",
        "\n",
        "### 1. Data\n",
        "  The type of data a neural network processes varies drastically based on the problem being solved. When we build a neural network, we define what shape and kind of data it can accept. It may sometimes be neccessary to modify our dataset so that it can be passed to our neural network.\n",
        "\n",
        "### 2. Layers\n",
        "  As we mentioned earlier each neural network consists of multiple layers. At each layer a different transformation of data occurs. Our initial input data is fed through the layers and eventually arrives at the output layer where we will obtain the result.\n",
        "\n",
        "  * Input Layer:\n",
        "  The input layer is the layer that our initial data is passed to. It is the first layer in our neural network.\n",
        "\n",
        "  * Output Layer:\n",
        "  The output layer is the layer that we will retrive our results from. Once the data has passed through all other layers it will arrive here.\n",
        "\n",
        "  * Hidden Layer(s):\n",
        "  All the other layers in our neural network are called \"hidden layers\". This is because they are hidden to us, we cannot observe them. Most neural networks consist of at least one hidden layer but can have an unlimited amount. Typically, the more complex the model the more hidden layers.\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:640/format:webp/0*BSxP3AHxBe_IevHC.png)\n",
        "\n",
        "### 3. Neurons\n",
        "\n",
        "Each layer is made up of what are called neurons. For example, say we want to pass an image that is 28x28 pixels, thats 784 pixels. We would need 784 neurons in our input layer to capture each of these pixels.\n",
        "\n",
        "### 4. Weights\n",
        "  Weights are associated with each connection in our neural network. Every pair of connected nodes will have one weight that denotes the strength of the connection between them. The model will try to determine what these weights should be to achieve the best result. Weights start out at a constant or random value and will change as the network sees training data.\n",
        "\n",
        "### 5. Biases\n",
        "\n",
        "  A bias is simply a constant value associated with each layer. It can be thought of as an extra neuron that has no connections. The purpose of a bias is to shift an entire activation function by a constant value. This allows a lot more flexibllity when it comes to choosing an activation and training the network. There is one bias for each layer.\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:960/1*0lejoYyyQWjYzEP_BNW2nw.jpeg)\n",
        "\n",
        "### 6. Activation Function\n",
        "\n",
        "Activation functions are simply a function that is applied to the weighed sum of a neuron. They can be anything we want but are typically higher order/degree functions that aim to add a higher dimension to our data. We would want to do this to introduce more complexity to our model.\n",
        "\n",
        "\n",
        "A list of some common activation functions and their graphs can be seen below:\n",
        "\n",
        "* #### Sigmoid function:\n",
        "\n",
        "  Transform $ (- \\infty $ to $ \\infty) $ into (0 to 1) range\n",
        "  $$\n",
        "  sigmoid(x) = \\frac{1}{1 + e^{-x}}\n",
        "  $$\n",
        "\n",
        "* #### Tanh function:\n",
        "\n",
        "  Similar to sigmoid, difference being that output is -1 to +1\n",
        "\n",
        "\n",
        "$$\n",
        "tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
        "$$\n",
        "\n",
        "  \n",
        "\n",
        "* #### ReLU\n",
        "\n",
        "  $$\n",
        "  f(x) = \\begin{cases} %\n",
        "                      0 & if \\; x<0 \\\\\n",
        "                      x &  if \\;x \\geq 0.\n",
        "                  \\end{cases}\n",
        "  $$\n",
        "\n",
        "![](https://www.researchgate.net/publication/327435257/figure/fig4/AS:742898131812354@1554132125449/Activation-Functions-ReLU-Tanh-Sigmoid.ppm)\n",
        "\n",
        "There are several other loss functions, each with its own specific use cases and characteristics, you can explore those at your own pace."
      ],
      "metadata": {
        "id": "KR9hR8ak7hgX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## How it Works\n",
        "\n",
        "A neural network consists of many Nodes (Neurons) in many layers. Each layer can have any number of nodes and a neural network can have any number of layers.\n",
        "\n",
        "\n",
        "\n",
        "Lets take the example of whats going on with a single node in the network.\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*SaQMHTLi4C7MIA4IzjAXJw.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "Y =(\\sum_{i=0}^n w_i x_i) + b\n",
        "\\end{equation}\n",
        "\n",
        " * w: stands for the weight of each connection to the neuron\n",
        "\n",
        " * x: stands for the value of the connected neuron from the previous value\n",
        "\n",
        " * b: stands for the bias at each layer, this is a constant\n",
        "\n",
        " * n: is the number of connections\n",
        "\n",
        " * Y: is the output of the current neuron\n",
        "\n",
        "\n",
        " The equation you just read is called a weighed sum. We will take this weighted sum at each and every neuron as we pass information through the network. Then we will add what's called a bias to this sum. The bias allows us to shift the network up or down by a constant value. It is like the y-intercept of a line.\n",
        "\n",
        "\n",
        " But that equation is the not complete one! We forgot a crucial part, the **activation function**. Our new equation with the addition of an activation function\n",
        " is seen below.\n",
        "\n",
        "\\begin{equation}\n",
        " Y =F((\\sum_{i=0}^n w_i x_i) + b)\n",
        "\\end{equation}\n",
        "\n"
      ],
      "metadata": {
        "id": "0dIWWXD45sHZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Every Neural Network has 2 main parts:\n",
        "\n",
        "* Forward Propagation.\n",
        "* Backward Propagation.\n",
        "\n",
        "![](https://www.researchgate.net/publication/327637282/figure/fig1/AS:670566579175436@1536886939536/A-simple-neural-network-with-two-hidden-layers-of-two-nodes-each-four-inputs-and-a.ppm)\n",
        "\n",
        "# Forward Propogation.\n",
        "\n",
        "* Forward propagation is the process in which input data is processed through the neural network's layers to produce an output.\n",
        "\n",
        "* It involves passing the input data through each layer of the neural network, applying the layer's weights and activation functions, until the final output is obtained.\n",
        "\n",
        "# Backpropagation\n",
        "Backpropagation is the fundemental algorithm behind training neural networks. It is what changes the weights and biases of our network. To fully explain this process, we need to learn something called a cost/loss function.\n",
        "\n",
        "# Loss/Cost Function\n",
        "\n",
        "For our training data we have the features (input) and the labels (expected output), because of this we can compare the output from our network to the expected output. Based on the difference between these values we can determine if our network has done a good job or poor job. If the network has done a good job, we'll make minor changes to the weights and biases. If it has done a poor job our changes may be more drastic.\n",
        "\n",
        "Some common loss/cost functions include.\n",
        "\n",
        "* Mean Squared Error\n",
        "\\begin{equation}\n",
        "y = \\sum_{i=1}^{D}(x_i-y_i)^2\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "* Mean Absolute Error\n",
        "\\begin{equation}y = \\sum_{i=1}^{D}|x_i-y_i|\n",
        "\\end{equation}\n",
        "* Hinge Loss\n",
        "\\begin{equation}\n",
        "y = max(0, 1 - y \\cdot \\hat{y})\n",
        "\\end{equation}\n",
        "\n",
        "Where 'D' represents the number of samples in the dataset.\n",
        "\n",
        "# Optimizer\n",
        "Optimization function is simply the function that implements the backpropagation algorithm described above. Here's a list of a few common ones.\n",
        "\n",
        "* Gradient Descent\n",
        "* Stochastic Gradient Descent\n",
        "* Mini-Batch Gradient Descent\n",
        "* Momentum\n",
        "* Nesterov Accelerated Gradient\n",
        "\n",
        "![](https://i.pinimg.com/originals/6f/d6/22/6fd62253592b42795c48dc570a17579c.gif)\n",
        "\n",
        "During backpropagation we calculate the total error at the output nodes and propagate these errors back through the network using Backpropagation to calculate the gradients. Then we use an optimization method such as Gradient Descent to adjust all weights in the network with an aim of reducing the error at the output layer.\n",
        "\n"
      ],
      "metadata": {
        "id": "gPpxXw9S-HZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a Nueral Network"
      ],
      "metadata": {
        "id": "UbEFZXhaMLHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "import random"
      ],
      "metadata": {
        "id": "83mCQD8QFPFH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset\n",
        "\n",
        "iris = load_iris()\n",
        "data, labels = iris.data[:,0:2], iris.data[:,2]\n",
        "\n",
        "num_samples = len(labels)  # size of our dataset\n",
        "\n",
        "# shuffle the dataset\n",
        "shuffle_order = np.random.permutation(num_samples)\n",
        "data = data[shuffle_order, :]\n",
        "labels = labels[shuffle_order]"
      ],
      "metadata": {
        "id": "VK2wHTQ1FHlB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like the 1-dimensional problem previously, we can still do linear regression, except now we have two variables and therefore two weights as well. Let's denote the input variables as x1 and x2 and instead of using m as the coefficient variable, let's use w1 and w2. So for linear regression, we would have the following function:\n",
        "\n",
        "$$\n",
        "f(X) = w_1 x_1 + w_2 x_2 + b\n",
        "$$\n",
        "\n",
        "\n",
        "For example, suppose set w = [0.2, 0.6] and b = -0.3. Let's calculate the resulting . We can program this as a function called \"weighted_sum\"."
      ],
      "metadata": {
        "id": "1JFlz0COF9Hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_sum(x, w, b):\n",
        "    return b + np.dot(w, x)\n",
        "\n",
        "# set our paramters - weights and bias\n",
        "w = [0.2, 0.6]\n",
        "b = -0.3\n",
        "\n",
        "# for example, let's use the first data point\n",
        "X, y = data, labels\n",
        "\n",
        "pred_y = [weighted_sum(x, w, b) for x in X]\n",
        "\n",
        "# let's print out the first prediction\n",
        "print(\"for x=[%0.2f, %0.2f], predicted = %0.2f, actual = %0.2f\" % (X[0][0], X[0][1], pred_y[0], y[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6u5YRiIvFctw",
        "outputId": "0a7fc94e-1142-4e15-80f9-037ffa9fd968"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for x=[6.60, 2.90], predicted = 2.76, actual = 4.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can evaluate the quality of our predictions using cost/loss functions. Lets use the sum-squared error function\n",
        "\n"
      ],
      "metadata": {
        "id": "Ay4a6YMhG6tJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sum squared error\n",
        "def cost_function(y_pred, y_actual):\n",
        "    return 0.5 * np.sum((y_actual-y_pred)**2)\n",
        "\n",
        "error = cost_function(pred_y, y)\n",
        "print(error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hysg_sq0FoKA",
        "outputId": "71d7600e-0595-44ee-c3b9-c44519e06d15"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313.50559999999996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# normalizing the data\n",
        "X = X / np.amax(X, axis=0)\n",
        "y = y / np.amax(y, axis=0)\n",
        "\n",
        "# randomly initializing w, b\n",
        "w, b = [random.random(), random.random()], random.random()\n",
        "\n",
        "# our function w*x + b\n",
        "def F(X, w, b):\n",
        "    # Convert w to a numpy array\n",
        "    w = np.array(w)\n",
        "    return np.sum(w*X, axis=1) + b\n",
        "\n",
        "# calculating error using cost function(Here we use Mean Squared Error)\n",
        "y_pred = F(X, w, b)\n",
        "init_cost = cost_function(y_pred, y)\n",
        "\n",
        "print(\"initial parameters: w1=%0.3f, w2=%0.3f, b=%0.3f\"%(w[0], w[1], b))\n",
        "print(\"initial cost = %0.3f\" % init_cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuZqQYNmHFUf",
        "outputId": "e3895a05-52e6-46ad-a094-836d11995954"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial parameters: w1=0.704, w2=0.686, b=0.394\n",
            "initial cost = 8901.653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the partial derivatives are:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial w_1} = - \\sum{x_1^i \\cdot (y^i - (w_1 x_1^i + w_2 x_2^i+ b))}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial w_2} = - \\sum{x_2^i \\cdot (y^i - (w_1 x_1^i + w_2 x_2^i+ b))}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial b} = - \\sum{y^i - (w_1 x_1^i + w_2 x_2^i+ b)}\n",
        "$$"
      ],
      "metadata": {
        "id": "kpR3WeBjJ2sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# implement partial derivatives of our parameters\n",
        "\n",
        "def dJdw1(X, y, w, b):\n",
        "    return -np.dot(X[:,0], y - F(X, w, b))\n",
        "\n",
        "def dJdw2(X, y, w, b):\n",
        "    return -np.dot(X[:,1], y - F(X, w, b))\n",
        "\n",
        "def dJdb(X, y, w, b):\n",
        "    return -np.sum(y - F(X, w, b))"
      ],
      "metadata": {
        "id": "CveoHZvHJYPO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aftet that, we use the following update rule, where we calculate the gradient and then adjust the parameters.\n",
        "\n",
        "$$\n",
        "w_1 = w_1 - \\alpha \\cdot \\frac{\\partial J}{\\partial w_i}\n",
        "$$\n",
        "\n",
        "$$\n",
        "w_2 = w_2 - \\alpha \\cdot \\frac{\\partial J}{\\partial w_2}\n",
        "$$\n",
        "\n",
        "$$\n",
        "b = b - \\alpha \\cdot \\frac{\\partial J}{\\partial b}\n",
        "$$"
      ],
      "metadata": {
        "id": "KEAv94RcKQGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# choose the learning rate parameter and number of iterations\n",
        "lr = 0.001\n",
        "n_iters = 2000\n",
        "\n",
        "# run through gradient descent\n",
        "errors = []\n",
        "for i in range(n_iters):\n",
        "    w[0] = w[0] - lr * dJdw1(X, y, w, b)\n",
        "    w[1] = w[1] - lr * dJdw2(X, y, w, b)\n",
        "    b = b - lr * dJdb(X, y, w, b)\n",
        "    y_pred = F(X, w, b)\n",
        "    j = cost_function(y_pred, y)\n",
        "    errors.append(j)"
      ],
      "metadata": {
        "id": "e4PwIPMpAvLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the error\n",
        "plt.plot(range(n_iters), errors, linewidth=2)\n",
        "plt.title(\"Cost by iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"iterations\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "Yj9y6jspLAVp",
        "outputId": "a1987fc8-9d56-4bca-8c32-31ce84275a12"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Error')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* At the beginning of training, the loss is typically high as the model's weights are randomly initialized,the model struggles to make accurate predictions, resulting in a high training loss.\n",
        "\n",
        "* As the training progresses, the loss generally decreases, the model adjusts its weights and biases to minimize the difference between its predictions and the actual target values."
      ],
      "metadata": {
        "id": "bQKAdGigLwoL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's implement the sigmoid function.\n",
        "\n"
      ],
      "metadata": {
        "id": "zGyf_gw2NMrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))"
      ],
      "metadata": {
        "id": "soOMzv5qLEEI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basically, a sigmoid just squashes any input it gets to between 0 and 1. So we now modify our basic function in the following way: instead of just outputting the weighted sum by itself,we now pass that through the sigmoid function.\n",
        "\n",
        "  So instead of y = (w_1 * x_1) + (w_2 * x_2) + b our function now looks like:\n",
        "\n",
        "$$\n",
        "y = \\frac{1}{1 + e^{-(w_1 x_1 + w_2 x_2 + b)}}\n",
        "$$"
      ],
      "metadata": {
        "id": "CtafACY5NYq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_sum(x, w, b):\n",
        "    return b + np.dot(w, x)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# our parameters\n",
        "w = [0.2, 0.6]\n",
        "b = -0.3\n",
        "\n",
        "X, y = data, labels\n",
        "\n",
        "# get weighted sum like before\n",
        "Z = [weighted_sum(x, w, b) for x in X]\n",
        "\n",
        "# now transform the weighted sums with a sigmoid\n",
        "y_pred = [sigmoid(z) for z in Z]\n",
        "\n",
        "# evaluate error\n",
        "error = cost_function(y_pred, y)\n",
        "print(error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXQ3uzNLOCah",
        "outputId": "5533b63e-25f1-43d4-e601-52214210c6b7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "829.3581552057614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function we posed above can already be considered a neural network. But let's complicate things a bit further, by adding a hidden layer. Neurons can be arranged in layers. So instead of having just two input neurons and an output neuron, let's place a layer of three neurons in the middle\n",
        "\n",
        "![](https://raw.githubusercontent.com/ml4a/ml4a/a8831f15b581f091d16003b0b61a68ed1bbbb770/assets/neuralnet.jpg)"
      ],
      "metadata": {
        "id": "yTsbzEwKOvQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W1 = np.random.randn(2, 3)\n",
        "W2 = np.random.randn(3, 1)\n",
        "\n",
        "print(\"W1=\", W1)\n",
        "print(\"W2=\", W2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-MoMt2YOkTL",
        "outputId": "ad111985-977f-4610-b0a9-577a855a557b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1= [[-2.36408202 -1.21471789 -2.18496436]\n",
            " [-0.37813735 -0.45266246  0.45389561]]\n",
            "W2= [[1.49064272]\n",
            " [0.28597226]\n",
            " [1.53400531]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### implementing a forward pass"
      ],
      "metadata": {
        "id": "cVWOhKp9PFM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X, y = data, labels\n",
        "\n",
        "# first layer weighted sum z\n",
        "z = np.dot(X, W1)\n",
        "\n",
        "# project z through non-linear sigmoid\n",
        "z = sigmoid(z)\n",
        "\n",
        "# do another dot product at end (sigmoid is omitted)\n",
        "y_pred = np.dot(z, W2)\n",
        "\n",
        "# what is our cost\n",
        "error = cost_function(y_pred, y)\n",
        "\n",
        "print('predicted %0.2f for example 0, actual %0.2f, total cost %0.2f'%(pred_y[0], y[0], error))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vaPe9UeO9m7",
        "outputId": "2be40479-db4b-4107-e085-c10307aed77a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicted 2.76 for example 0, actual 4.60, total cost 193690.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's turn the above operations into a class.\n",
        "\n"
      ],
      "metadata": {
        "id": "J8iY5h-4PYpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Neural_Network(object):\n",
        "    def __init__(self, n0, n1, n2):\n",
        "        self.n0 = n0\n",
        "        self.n1 = n1\n",
        "        self.n2 = n2\n",
        "\n",
        "        # initialize weights\n",
        "        self.W1 = np.random.randn(self.n0, self.n1)\n",
        "        self.W2 = np.random.randn(self.n1 ,self.n2)\n",
        "\n",
        "    def predict(self, x):\n",
        "        z = np.dot(x, self.W1)\n",
        "        z = sigmoid(z)\n",
        "        y = np.dot(z, self.W2)\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "6F2SHwt0PDIW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate a neural network with 2 input neurons, 3 hidden neurons, and 1 output neuron using above class\n",
        "\n",
        "net = Neural_Network(2, 3, 1)"
      ],
      "metadata": {
        "id": "il41fIVZPSBb"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now to do a forward pass, we can simply run the networks predict function:\n"
      ],
      "metadata": {
        "id": "rxCMltIMPwST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = data, labels\n",
        "y_pred = net.predict(X)\n",
        "error = cost_function(y_pred, y)\n",
        "\n",
        "print('predicted %0.2f for example 0, actual %0.2f, total cost %0.2f'%(pred_y[0], y[0], error))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P87xH2HPrWt",
        "outputId": "9ce12282-ed78-4cd4-8adb-1a7938626517"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicted 2.76 for example 0, actual 4.60, total cost 302507.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " we have a 2x3x1 neural network with 9 weights and 4 biases for 13 total parameters.\n",
        "\n",
        " Now we optimize the parameters to minimize our cost function using Gradient Descent.\n",
        "\n",
        "Gradient Descent will find the gradient of the cost/loss function(J)\n",
        " with respect to the parameters w,b.\n",
        "\n",
        " $$\n",
        " w_i := w_i - \\alpha \\cdot \\frac{\\partial J}{\\partial w_i}\n",
        " $$"
      ],
      "metadata": {
        "id": "mDMiAPzJQHQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gradient(net, X, y):\n",
        "    w_delta = 1e-8\n",
        "\n",
        "    # get the current value of the loss, wherever the parameters are\n",
        "    y_pred_current = net.predict(X)\n",
        "    error_current = cost_function(y_pred_current, y)\n",
        "\n",
        "    # grab the current weights and copy them (so we can restore them after modification)\n",
        "    dw1, dw2 = np.zeros((net.n0, net.n1)), np.zeros((net.n1, net.n2))\n",
        "    W1, W2 = np.copy(net.W1), np.copy(net.W2)\n",
        "\n",
        "    # Calculate gradient for the first layer\n",
        "    for i in range(net.n0):\n",
        "        for j in range(net.n1):\n",
        "            net.W1 = np.copy(W1)\n",
        "            net.W1[i][j] += w_delta\n",
        "            y_pred = net.predict(X)\n",
        "            error = cost_function(y_pred, y)\n",
        "            dw1[i][j] = (error - error_current) / w_delta\n",
        "\n",
        "    # Calculate gradient for the second layer\n",
        "    for i in range(net.n1):\n",
        "        for j in range(net.n2):\n",
        "            net.W2 = np.copy(W2)\n",
        "            net.W2[i][j] += w_delta\n",
        "            y_pred = net.predict(X)\n",
        "            error = cost_function(y_pred, y)\n",
        "            dw2[i][j] = (error - error_current) / w_delta\n",
        "\n",
        "    # restore the original weights\n",
        "    net.W1, net.W2 = np.copy(W1), np.copy(W2)\n",
        "\n",
        "    return dw1, dw2"
      ],
      "metadata": {
        "id": "eXEVFmcoP05N"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above function 'get_gradient' calculates the gradient of a 2-layer network net, for our dataset X, y"
      ],
      "metadata": {
        "id": "ZUNZoWSBSfUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we train the network by the following steps:\n",
        "\n",
        "1. Load our dataset\n",
        "2. Instantiate a neural network\n",
        "3. Train it on the data using the gradient method made above."
      ],
      "metadata": {
        "id": "5U_VzLU4Sp_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data and labels\n",
        "X, y = data, labels.reshape((len(labels),1))\n",
        "\n",
        "# it's always a good idea to normalize the data between 0 and 1\n",
        "X = X/np.amax(X, axis=0)\n",
        "y = y/np.amax(y, axis=0)\n",
        "\n",
        "# create a 2x3x1 neural net\n",
        "net = Neural_Network(2, 3, 1)\n",
        "\n",
        "# what is the current cost?\n",
        "y_orig = net.predict(X)\n",
        "init_cost = cost_function(y_orig, y)\n",
        "print(\"initial cost = %0.3f\" % init_cost)\n",
        "\n",
        "# Set the learning rate, and how many epochs (updates) to try\n",
        "n_epochs = 2000\n",
        "learning_rate = 0.01\n",
        "\n",
        "# for each epoch, calculate the gradient, then subtract it from the parameters, and save the cost\n",
        "errors = []\n",
        "for i in range(n_epochs):\n",
        "    dw1, dw2 = get_gradient(net, X, y)\n",
        "    net.W1 = net.W1 - learning_rate * dw1\n",
        "    net.W2 = net.W2 - learning_rate * dw2\n",
        "    y_pred = net.predict(X)\n",
        "    error = cost_function(y_pred, y)\n",
        "    errors.append(error)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hw5Q_cWWSY45",
        "outputId": "dcd4ac31-c56f-4139-939c-3762d32c733c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial cost = 328.193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting iterations vs error\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(0, len(errors)), errors)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Error')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "YfzAFsVoTHgH",
        "outputId": "b4d9dc86-cfe0-4cf1-d2f3-c64629375770"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Error')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# -----------------------------\n",
        "# Activation functions\n",
        "# -----------------------------\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_derivative(a):\n",
        "    return (a > 0).astype(float)\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # stability\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Forward pass\n",
        "# -----------------------------\n",
        "def forward_pass(X, params):\n",
        "    W1, b1, W2, b2 = params[\"W1\"], params[\"b1\"], params[\"W2\"], params[\"b2\"]\n",
        "    z1 = X @ W1 + b1\n",
        "    a1 = relu(z1)\n",
        "    z2 = a1 @ W2 + b2\n",
        "    a2 = softmax(z2)\n",
        "    cache = (X, z1, a1, z2, a2)\n",
        "    return a2, cache\n",
        "\n",
        "# -----------------------------\n",
        "# Loss (cross-entropy)\n",
        "# -----------------------------\n",
        "def compute_loss(y_true, y_pred):\n",
        "    m = y_true.shape[0]\n",
        "    log_likelihood = -np.log(y_pred[range(m), y_true] + 1e-9)  # avoid log(0)\n",
        "    return np.sum(log_likelihood) / m\n",
        "\n",
        "# -----------------------------\n",
        "# Gradient function\n",
        "# -----------------------------\n",
        "def get_gradient(X, y, params):\n",
        "    W1, b1, W2, b2 = params[\"W1\"], params[\"b1\"], params[\"W2\"], params[\"b2\"]\n",
        "    m = X.shape[0]\n",
        "\n",
        "    # Forward\n",
        "    y_pred, cache = forward_pass(X, params)\n",
        "    X, z1, a1, z2, _ = cache\n",
        "\n",
        "    # One-hot encoding for labels\n",
        "    y_onehot = np.zeros_like(y_pred)\n",
        "    y_onehot[np.arange(m), y] = 1\n",
        "\n",
        "    # Backpropagation\n",
        "    dz2 = y_pred - y_onehot\n",
        "    dW2 = (a1.T @ dz2) / m\n",
        "    db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
        "\n",
        "    dz1 = (dz2 @ W2.T) * relu_derivative(a1)\n",
        "    dW1 = (X.T @ dz1) / m\n",
        "    db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
        "\n",
        "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
        "    return grads\n",
        "\n",
        "# -----------------------------\n",
        "# Initialize parameters (He init for ReLU)\n",
        "# -----------------------------\n",
        "def init_network(input_dim, hidden_dim, output_dim):\n",
        "    np.random.seed(42)\n",
        "    W1 = np.random.randn(input_dim, hidden_dim) * np.sqrt(2.0 / input_dim)\n",
        "    b1 = np.zeros((1, hidden_dim))\n",
        "    W2 = np.random.randn(hidden_dim, output_dim) * np.sqrt(2.0 / hidden_dim)\n",
        "    b2 = np.zeros((1, output_dim))\n",
        "    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
        "\n",
        "# -----------------------------\n",
        "# Training loop\n",
        "# -----------------------------\n",
        "def train(X, y, input_dim, hidden_dim, output_dim, lr=0.1, epochs=2000):\n",
        "    params = init_network(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Forward\n",
        "        y_pred, _ = forward_pass(X, params)\n",
        "        loss = compute_loss(y, y_pred)\n",
        "\n",
        "        # Gradients\n",
        "        grads = get_gradient(X, y, params)\n",
        "\n",
        "        # Update\n",
        "        params[\"W1\"] -= lr * grads[\"dW1\"]\n",
        "        params[\"b1\"] -= lr * grads[\"db1\"]\n",
        "        params[\"W2\"] -= lr * grads[\"dW2\"]\n",
        "        params[\"b2\"] -= lr * grads[\"db2\"]\n",
        "\n",
        "        # Print progress\n",
        "        if epoch % 200 == 0:\n",
        "            acc = np.mean(np.argmax(y_pred, axis=1) == y)\n",
        "            print(f\"Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {acc:.4f}\")\n",
        "\n",
        "    return params\n",
        "\n",
        "# -----------------------------\n",
        "# Prediction function\n",
        "# -----------------------------\n",
        "def predict(X, params):\n",
        "    y_pred, _ = forward_pass(X, params)\n",
        "    return np.argmax(y_pred, axis=1)\n",
        "\n",
        "# -----------------------------\n",
        "# Example run (XOR dataset)\n",
        "# -----------------------------\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([0,1,1,0])  # XOR labels\n",
        "\n",
        "# Train (2 input features, 16 hidden units, 2 output classes)\n",
        "trained_params = train(X, y, input_dim=2, hidden_dim=16, output_dim=2, lr=0.1, epochs=5000)\n",
        "\n",
        "# Final accuracy\n",
        "y_pred = predict(X, trained_params)\n",
        "print(\"Final Predictions:\", y_pred)\n",
        "print(\"Final Accuracy:\", np.mean(y_pred == y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdG8ZQjjBr83",
        "outputId": "0fb6b1b6-0f21-4290-edef-72b4d0da5105"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.6832, Accuracy: 0.7500\n",
            "Epoch 200, Loss: 0.0718, Accuracy: 1.0000\n",
            "Epoch 400, Loss: 0.0265, Accuracy: 1.0000\n",
            "Epoch 600, Loss: 0.0148, Accuracy: 1.0000\n",
            "Epoch 800, Loss: 0.0099, Accuracy: 1.0000\n",
            "Epoch 1000, Loss: 0.0073, Accuracy: 1.0000\n",
            "Epoch 1200, Loss: 0.0057, Accuracy: 1.0000\n",
            "Epoch 1400, Loss: 0.0046, Accuracy: 1.0000\n",
            "Epoch 1600, Loss: 0.0039, Accuracy: 1.0000\n",
            "Epoch 1800, Loss: 0.0033, Accuracy: 1.0000\n",
            "Epoch 2000, Loss: 0.0029, Accuracy: 1.0000\n",
            "Epoch 2200, Loss: 0.0025, Accuracy: 1.0000\n",
            "Epoch 2400, Loss: 0.0023, Accuracy: 1.0000\n",
            "Epoch 2600, Loss: 0.0020, Accuracy: 1.0000\n",
            "Epoch 2800, Loss: 0.0019, Accuracy: 1.0000\n",
            "Epoch 3000, Loss: 0.0017, Accuracy: 1.0000\n",
            "Epoch 3200, Loss: 0.0016, Accuracy: 1.0000\n",
            "Epoch 3400, Loss: 0.0015, Accuracy: 1.0000\n",
            "Epoch 3600, Loss: 0.0014, Accuracy: 1.0000\n",
            "Epoch 3800, Loss: 0.0013, Accuracy: 1.0000\n",
            "Epoch 4000, Loss: 0.0012, Accuracy: 1.0000\n",
            "Epoch 4200, Loss: 0.0011, Accuracy: 1.0000\n",
            "Epoch 4400, Loss: 0.0011, Accuracy: 1.0000\n",
            "Epoch 4600, Loss: 0.0010, Accuracy: 1.0000\n",
            "Epoch 4800, Loss: 0.0009, Accuracy: 1.0000\n",
            "Final Predictions: [0 1 1 0]\n",
            "Final Accuracy: 1.0\n"
          ]
        }
      ]
    }
  ]
}